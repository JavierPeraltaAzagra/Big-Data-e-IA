{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aTPRfoZDxvau",
      "metadata": {
        "id": "aTPRfoZDxvau"
      },
      "source": [
        "# UT1 — Integración, procesamiento y análisis con Python + PySpark\n",
        "\n",
        "**Módulo:** Sistemas de Big Data  \n",
        "**Unidad:** UT1 - Aplicación de técnicas de integración, procesamiento y análisis de la información  \n",
        "**Fecha:** 2025-10-06\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xtzb7b_bxvau",
      "metadata": {
        "id": "xtzb7b_bxvau"
      },
      "source": [
        "## 0) Comprobación de entorno\n",
        "\n",
        "Ejecuta esta celda. Si no hay PySpark instalado, verás un aviso con instrucciones. Si existe, se inicializa una `SparkSession` local.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sw5lFZmCxvav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw5lFZmCxvav",
        "outputId": "79c58051-e738-48a3-ad6f-a466e9a87fc9"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession, functions \u001b[38;5;28;01mas\u001b[39;00m F, types \u001b[38;5;28;01mas\u001b[39;00m T\n\u001b[32m      7\u001b[39m spark = (\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m         \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUT1_ETL_MVP\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m         \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m         \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.shuffle.partitions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m         \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark version:\u001b[39m\u001b[33m\"\u001b[39m, spark.version)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMaster:\u001b[39m\u001b[33m\"\u001b[39m, spark.sparkContext.master)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Ejecuta esta celda primero\n",
        "import sys, os, time\n",
        "\n",
        "try:\n",
        "    import pyspark  # type: ignore\n",
        "    from pyspark.sql import SparkSession, functions as F, types as T\n",
        "    spark = (SparkSession.builder\n",
        "             .appName(\"UT1_ETL_MVP\")\n",
        "             .master(\"local[*]\")\n",
        "             .config(\"spark.sql.shuffle.partitions\",\"8\")\n",
        "             .getOrCreate())\n",
        "    print(\"Spark version:\", spark.version)\n",
        "    print(\"Master:\", spark.sparkContext.master)\n",
        "except Exception as e:\n",
        "    print(\"Error inicializando Spark:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zmz_Mojoxvav",
      "metadata": {
        "id": "zmz_Mojoxvav"
      },
      "source": [
        "## 1) Generación de dataset sintético (e-commerce)\n",
        "\n",
        "Crearemos un CSV con **pedidos** y **líneas** para simular un caso real sin datos personales (RGPD OK).  \n",
        "Volumen objetivo por defecto: ~**200.000** filas en `order_items.csv` (ajustable).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "lOi-IfkYxvav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOi-IfkYxvav",
        "outputId": "467cc72c-ecfd-4efa-b55a-ebbea97acb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV generados en: C:\\datos\\Documents\\Big Data e IA\\Sistemas de Big Data\\Ejercicio 2\\data\\raw\n"
          ]
        }
      ],
      "source": [
        "# Parámetros de generación (puedes ajustar el volumen si tu equipo es modesto)\n",
        "import os, csv, random, math, datetime, itertools\n",
        "from pathlib import Path\n",
        "\n",
        "random.seed(42)\n",
        "out_dir = Path(\"data/raw\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "N_ORDERS = 25000     # ~25k pedidos\n",
        "MAX_ITEMS = 12       # hasta 12 líneas por pedido\n",
        "\n",
        "products = [\n",
        "    (\"P-100\", \"Teclado mecánico\", 59.90),\n",
        "    (\"P-101\", \"Ratón gaming\", 39.90),\n",
        "    (\"P-102\", \"Monitor 27\\\"\", 199.00),\n",
        "    (\"P-103\", \"Auriculares\", 79.00),\n",
        "    (\"P-104\", \"Webcam HD\", 49.90),\n",
        "    (\"P-105\", \"SSD 1TB\", 89.00),\n",
        "    (\"P-106\", \"Silla ergonómica\", 149.00),\n",
        "    (\"P-107\", \"Alfombrilla XL\", 19.90),\n",
        "]\n",
        "\n",
        "countries = [\"ES\",\"FR\",\"DE\",\"IT\",\"PT\"]\n",
        "states = [\"NEW\",\"PAID\",\"SHIPPED\",\"DELIVERED\",\"RETURNED\",\"CANCELLED\"]\n",
        "\n",
        "start_date = datetime.date(2025, 1, 1)\n",
        "def rand_date():\n",
        "    delta = random.randint(0, 250)\n",
        "    return start_date + datetime.timedelta(days=delta)\n",
        "\n",
        "path_orders = out_dir / \"orders.csv\"\n",
        "path_items  = out_dir / \"order_items.csv\"\n",
        "\n",
        "with path_orders.open(\"w\", newline=\"\", encoding=\"utf-8\") as f1,      path_items.open(\"w\", newline=\"\", encoding=\"utf-8\") as f2:\n",
        "    w1 = csv.writer(f1)\n",
        "    w2 = csv.writer(f2)\n",
        "    w1.writerow([\"order_id\",\"order_date\",\"country\",\"state\",\"customer_age\"])\n",
        "    w2.writerow([\"order_id\",\"item_id\",\"product_id\",\"product_name\",\"unit_price\",\"qty\"])\n",
        "\n",
        "    oid = 100000\n",
        "    for _ in range(N_ORDERS):\n",
        "        d = rand_date()\n",
        "        country = random.choice(countries)\n",
        "        state = random.choices(states, weights=[0.15,0.25,0.25,0.2,0.05,0.10])[0]\n",
        "        age = max(16, int(random.gauss(35, 10)))\n",
        "        w1.writerow([oid, d.isoformat(), country, state, age])\n",
        "\n",
        "        k = random.randint(1, MAX_ITEMS)\n",
        "        for i in range(1, k+1):\n",
        "            prod_id, prod_name, price = random.choice(products)\n",
        "            qty = max(1, int(random.gauss(2, 1)))\n",
        "            w2.writerow([oid, i, prod_id, prod_name, price, qty])\n",
        "\n",
        "        oid += 1\n",
        "\n",
        "print(\"CSV generados en:\", out_dir.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e_eu5ayDxvav",
      "metadata": {
        "id": "e_eu5ayDxvav"
      },
      "source": [
        "## 2) ETL con **PySpark**: ingestión → limpieza/validación → enriquecimiento → **Parquet particionado**\n",
        "\n",
        "**Objetivo:** crear un *silver dataset* en `data/processed/` particionado por `order_year=YYYY`.\n",
        "\n",
        "**Requisitos mínimos (MVP):**\n",
        "- Esquema explícito para `orders` y `order_items`.\n",
        "- Validaciones básicas (valores nulos, rangos, duplicados).\n",
        "- Enriquecimiento: `order_total` por pedido y `order_month`.\n",
        "- Escritura en **Parquet** particionado por `order_year`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WbXr1Tb5xvav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbXr1Tb5xvav",
        "outputId": "3139a832-c1ef-4216-cd36-58c670c2d536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orders: 25000 Items: 163363\n",
            "Duplicados en items: 0\n",
            "Parquet escrito en: data/processed/orders_parquet\n",
            "Métricas: {'orders': 25000, 'items': 163363, 'orders_with_total': 25000, 'nulls_in_state': 0}\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "# 2.1 Definir esquemas\n",
        "orders_schema = T.StructType([\n",
        "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"order_date\", T.StringType(), False),\n",
        "    T.StructField(\"country\", T.StringType(), False),\n",
        "    T.StructField(\"state\", T.StringType(), False),\n",
        "    T.StructField(\"customer_age\", T.IntegerType(), True),\n",
        "])\n",
        "\n",
        "items_schema = T.StructType([\n",
        "    T.StructField(\"order_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"item_id\", T.IntegerType(), False),\n",
        "    T.StructField(\"product_id\", T.StringType(), False),\n",
        "    T.StructField(\"product_name\", T.StringType(), True),\n",
        "    T.StructField(\"unit_price\", T.DoubleType(), True),\n",
        "    T.StructField(\"qty\", T.IntegerType(), True),\n",
        "])\n",
        "\n",
        "raw_dir = \"data/raw\"\n",
        "df_orders = (spark.read\n",
        "    .option(\"header\", True)\n",
        "    .schema(orders_schema)\n",
        "    .csv(f\"{raw_dir}/orders.csv\"))\n",
        "\n",
        "df_items = (spark.read\n",
        "    .option(\"header\", True)\n",
        "    .schema(items_schema)\n",
        "    .csv(f\"{raw_dir}/order_items.csv\"))\n",
        "\n",
        "print(\"Orders:\", df_orders.count(), \"Items:\", df_items.count())\n",
        "\n",
        "# 2.2 Limpieza / validaciones\n",
        "# - Quitar pedidos sin líneas (join anti) y líneas con precio o cantidad inválida\n",
        "df_items = df_items.where((F.col(\"unit_price\") > 0) & (F.col(\"qty\") > 0))\n",
        "\n",
        "# - Parsear fechas y derivar columnas\n",
        "df_orders = (df_orders\n",
        "    .withColumn(\"order_ts\", F.to_timestamp(\"order_date\", \"yyyy-MM-dd\"))\n",
        "    .withColumn(\"order_year\", F.year(\"order_ts\"))\n",
        "    .withColumn(\"order_month\", F.date_format(\"order_ts\", \"yyyy-MM\"))\n",
        ")\n",
        "\n",
        "# - Duplicados (métrica + deduplicación por order_id,item_id)\n",
        "dup_count = df_items.count() - df_items.dropDuplicates([\"order_id\",\"item_id\"]).count()\n",
        "print(\"Duplicados en items:\", dup_count)\n",
        "df_items = df_items.dropDuplicates([\"order_id\",\"item_id\"])\n",
        "\n",
        "# 2.3 Enriquecimiento\n",
        "df_fact = (df_items\n",
        "    .join(df_orders, \"order_id\", \"inner\")\n",
        "    .withColumn(\"line_total\", F.col(\"unit_price\") * F.col(\"qty\"))\n",
        ")\n",
        "\n",
        "df_order_totals = (df_fact\n",
        "    .groupBy(\"order_id\",\"order_year\",\"order_month\",\"country\",\"state\")\n",
        "    .agg(F.sum(\"line_total\").alias(\"order_total\"))\n",
        ")\n",
        "\n",
        "# 2.4 Escritura en Parquet particionado\n",
        "out_dir = \"data/processed/orders_parquet\"\n",
        "(df_order_totals\n",
        "    .repartition(\"order_year\")\n",
        "    .write\n",
        "    .mode(\"overwrite\")\n",
        "    .partitionBy(\"order_year\")\n",
        "    .parquet(out_dir)\n",
        ")\n",
        "print(\"Parquet escrito en:\", out_dir)\n",
        "\n",
        "# 2.5 Métricas de calidad mínimas (ejemplo)\n",
        "metrics = {\n",
        "    \"orders\": df_orders.count(),\n",
        "    \"items\": df_items.count(),\n",
        "    \"orders_with_total\": df_order_totals.count(),\n",
        "    \"nulls_in_state\": df_orders.where(F.col(\"state\").isNull()).count(),\n",
        "}\n",
        "\n",
        "print(\"Métricas:\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dgwha0AYxvav",
      "metadata": {
        "id": "Dgwha0AYxvav"
      },
      "source": [
        "## 3) Spark SQL — consultas de negocio\n",
        "**Ejemplos**\n",
        "- **Ingresos por mes y país**\n",
        "- **Top 5 países por ingreso total**\n",
        "- **Distribución de estados del pedido**\n",
        "\n",
        "**Actividad**\n",
        "- **Ticket medio y mediana por mes y país**\n",
        "- **Tasas de devolución y cancelación por mes y país**\n",
        "- **Crecimiento mes a mes (MoM) de ingresos por país**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yo6BEraHxvav",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo6BEraHxvav",
        "outputId": "b50e7cf2-d759-41bc-be63-186bcda355c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+\n",
            "|order_month|country|revenue |\n",
            "+-----------+-------+--------+\n",
            "|2025-01    |DE     |608494.1|\n",
            "|2025-01    |ES     |566387.1|\n",
            "|2025-01    |FR     |574235.6|\n",
            "|2025-01    |IT     |599407.3|\n",
            "|2025-01    |PT     |543649.5|\n",
            "|2025-02    |DE     |504489.6|\n",
            "|2025-02    |ES     |561997.8|\n",
            "|2025-02    |FR     |516163.8|\n",
            "|2025-02    |IT     |487299.9|\n",
            "|2025-02    |PT     |531820.6|\n",
            "|2025-03    |DE     |608561.2|\n",
            "|2025-03    |ES     |605323.3|\n",
            "|2025-03    |FR     |580193.2|\n",
            "|2025-03    |IT     |564331.8|\n",
            "|2025-03    |PT     |612236.1|\n",
            "|2025-04    |DE     |517544.0|\n",
            "|2025-04    |ES     |585974.6|\n",
            "|2025-04    |FR     |535885.1|\n",
            "|2025-04    |IT     |552092.1|\n",
            "|2025-04    |PT     |556325.2|\n",
            "+-----------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+-------------+\n",
            "|country|total_revenue|\n",
            "+-------+-------------+\n",
            "|PT     |4778615.1    |\n",
            "|ES     |4746869.9    |\n",
            "|DE     |4708763.8    |\n",
            "|IT     |4703156.6    |\n",
            "|FR     |4617584.2    |\n",
            "+-------+-------------+\n",
            "\n",
            "+---------+------+\n",
            "|state    |orders|\n",
            "+---------+------+\n",
            "|PAID     |6269  |\n",
            "|SHIPPED  |6183  |\n",
            "|DELIVERED|4955  |\n",
            "|NEW      |3862  |\n",
            "|CANCELLED|2504  |\n",
            "|RETURNED |1227  |\n",
            "+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Crea vista temporal y ejecuta SQL\n",
        "df_order_totals.createOrReplaceTempView(\"orders_totals\")\n",
        "\n",
        "q1 = spark.sql(\"\"\"SELECT order_month, country, ROUND(SUM(order_total), 2) AS revenue\n",
        "FROM orders_totals\n",
        "GROUP BY order_month, country\n",
        "ORDER BY order_month, country\n",
        "\"\"\")\n",
        "q1.show(20, truncate=False)\n",
        "\n",
        "q2 = spark.sql(\"\"\"SELECT country, ROUND(SUM(order_total), 2) AS total_revenue\n",
        "FROM orders_totals\n",
        "GROUP BY country\n",
        "ORDER BY total_revenue DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "q2.show(truncate=False)\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "q3 = (spark.table(\"orders_totals\")\n",
        "        .groupBy(\"state\")\n",
        "        .agg(F.countDistinct(\"order_id\").alias(\"orders\"))\n",
        "        .orderBy(F.desc(\"orders\")))\n",
        "q3.show(truncate=False)\n",
        "\n",
        "#TODO: Consultas 4, 5, 6. q4, q5, q6\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
